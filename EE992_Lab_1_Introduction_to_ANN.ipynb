{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **EE992 Lab 1**\n",
        "# Intro to Deep Learning Lab\n",
        "\n",
        "In this lab we are going to basically fork an exisiting DL basics introduction and find out the issues with doing so. Enabling you to troubleshoot these issue later in the class yourself.\n",
        "Along with that by the end of this lab you should be able to build a NN to solve classification tasks.\n"
      ],
      "metadata": {
        "id": "PM4pABlrZnbS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Only required if you want to save you outputs easily.\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lI1ZMuT2YVd1",
        "outputId": "f72539bc-5c0e-4862-e0f8-c40d86cfde87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test to see if the instance is up and running correctly, checking both the CPU instance and GPU instance."
      ],
      "metadata": {
        "id": "Esd2nzqJpDdQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /proc/cpuinfo # gives details of CPU\n",
        "\n",
        "!nvidia-smi # gives details of GPU\n"
      ],
      "metadata": {
        "id": "WWbQty2XpCzi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ex6NqM4lKaRg"
      },
      "source": [
        "## Part 0: Prerequisites:\n",
        "\n",
        "We recommend that you run this this notebook in the cloud on Google Colab (see link with icon at the top) if you're not already doing so. It's the simplest way to get started. You can also [install TensorFlow locally](https://www.tensorflow.org/install/). But, again, simple is best (with caveats):\n",
        "\n",
        "[tf.keras](https://www.tensorflow.org/guide/keras) is the simplest way to build and train neural network models in TensorFlow. So, that's what we'll stick with in this tutorial, unless the models neccessitate a lower-level API.\n",
        "\n",
        "Note that there's [tf.keras](https://www.tensorflow.org/guide/keras) (comes with TensorFlow) and there's [Keras](https://keras.io/) (standalone). You should be using [tf.keras](https://www.tensorflow.org/guide/keras) because (1) it comes with TensorFlow so you don't need to install anything extra and (2) it comes with powerful TensorFlow-specific features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3FgyF_4wKaRg"
      },
      "outputs": [],
      "source": [
        "# !pip uninstall tensorflow  #just incase you want to change version\n",
        "# !pip install tensorflow==2.X.0\n",
        "# TensorFlow and tf.keras\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Flatten, Dense\n",
        "\n",
        "# Commonly used modules\n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# Images, plots, display, and visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import IPython\n",
        "\n",
        "\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3z9JZ-KKaRi"
      },
      "source": [
        "## Part 1: Generic Simple ANN:\n",
        "### Boston Housing Price Prediction with Feed Forward Neural Networks\n",
        "\n",
        "Let's start with using a fully-connected neural network to do predict housing prices. The following image highlights the difference between regression and classification (see part 2). Given an observation as input, **regression** outputs a continuous value (e.g., exact temperature) and classificaiton outputs a class/category that the observation belongs to.\n",
        "\n",
        "<img src=\"https://i.imgur.com/vvSoAzg.jpg\" alt=\"classification_regression\" width=\"400\"/>\n",
        "\n",
        "For the Boston housing dataset, we get 506 rows of data, with 13 features in each. Our task is to build a regression model that takes these 13 features as input and output a single value prediction of the \"median value of owner-occupied homes (in $1000).\"\n",
        "\n",
        "Now, we load the dataset. Loading the dataset returns four NumPy arrays:\n",
        "\n",
        "* The `train_features` and `train_labels` arrays are the *training set*—the data the model uses to learn.\n",
        "* The model is tested against the *test set*, the `test_features`, and `test_labels` arrays."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QTX7MBKxKaRj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3afc4d7-4c4b-4580-817c-e1a9843c6a96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/boston_housing.npz\n",
            "57026/57026 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "# Load the Boston Housing dataset from the Keras library and split into training and testing sets\n",
        "(train_features, train_labels), (test_features, test_labels) = keras.datasets.boston_housing.load_data()\n",
        "\n",
        "# Get per-feature statistics (mean, standard deviation) from the training set to normalize by\n",
        "train_mean = np.mean(train_features, axis=0)\n",
        "train_std = np.std(train_features, axis=0)\n",
        "\n",
        "# Normalize the training features by subtracting the mean and dividing by the standard deviation\n",
        "train_features = (train_features - train_mean) / train_std"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3QQZBvx2KaRk"
      },
      "source": [
        "### Build the model\n",
        "\n",
        "Building the neural network requires configuring the layers of the model, then compiling the model. First we stack a few layers together using `keras.Sequential`. Next we configure the loss function, optimizer, and metrics to monitor. These are added during the model's compile step:\n",
        "\n",
        "* *Loss function* - measures how accurate the model is during training, we want to minimize this with the optimizer.\n",
        "* *Optimizer* - how the model is updated based on the data it sees and its loss function.\n",
        "* *Metrics* - used to monitor the training and testing steps.\n",
        "\n",
        "Let's build a network with 1 hidden layer of 20 neurons, and use mean squared error (MSE) as the loss function (most common one for regression problems):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N8fPA0CMKaRk"
      },
      "outputs": [],
      "source": [
        "\n",
        "def build_model():\n",
        "    # Define a simple neural network model\n",
        "    model = keras.Sequential([\n",
        "        # First dense layer with 20 neurons and a ReLU activation function\n",
        "        Dense(20, activation=tf.nn.relu, input_shape=[len(train_features[0])]),\n",
        "        # Second dense layer with 1 neuron\n",
        "        Dense(1)\n",
        "    ])\n",
        "\n",
        "    # Compile the model with Adam optimizer, mean squared error loss,\n",
        "    # and metrics for mean absolute error and mean squared error\n",
        "    model.compile(optimizer=tf.train.AdamOptimizer(),\n",
        "                  loss='mse',\n",
        "                  metrics=['mae', 'mse'])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "M0xsaN2oebsb"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FGS5rHEKaRl"
      },
      "source": [
        "### Train the model\n",
        "\n",
        "Training the neural network model requires the following steps:\n",
        "\n",
        "1. Feed the training data to the model—in this example, the `train_features` and `train_labels` arrays.\n",
        "2. The model learns to associate features and labels.\n",
        "3. We ask the model to make predictions about a test set—in this example, the `test_features` array. We verify that the predictions match the labels from the `test_labels` array.\n",
        "\n",
        "To start training,  call the `model.fit` method—the model is \"fit\" to the training data:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*In this code, I have defined a callback class called 'PrintDot' that helps make the output of the training less verbose but still shows progress by printing a dot after every epoch. The model is built by calling the 'build_model' function, then an early stopping callback is defined to stop training if the validation loss does not improve for 50 epochs. The model is then trained using the train_features and train_labels, for 100 epochs, with a verbosity level of 1, and a validation split of 10%. The callbacks for early stopping and printing progress are passed to the fit function. The training history is then stored in a pandas DataFrame, and the final root mean square error is calculated on the validation set and printed.*"
      ],
      "metadata": {
        "id": "TmPTpl4wrJ61"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-HnDDXJWKaRl"
      },
      "outputs": [],
      "source": [
        "# Define a callback class to print the progress of the training\n",
        "class PrintDot(keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs):\n",
        "        if epoch % 100 == 0:\n",
        "            print('') # Print a new line after every 100 epochs\n",
        "        print('.', end='') # Print a dot to indicate progress\n",
        "\n",
        "# Build the model\n",
        "model = build_model()\n",
        "\n",
        "# Define early stopping callback to stop training if the validation loss does not improve for 50 epochs\n",
        "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=50)\n",
        "\n",
        "# Train the model with the training data, for 100 epochs, with verbosity level 0, validation split of 10%, and callbacks for early stopping and printing progress\n",
        "history = model.fit(train_features, train_labels, epochs=100, verbose=0, validation_split = 0.1,\n",
        "                    callbacks=[early_stop, PrintDot()])\n",
        "\n",
        "# Create a DataFrame from the history of the training\n",
        "hist = pd.DataFrame(history.history)\n",
        "hist['epoch'] = history.epoch\n",
        "\n",
        "# Calculate the final root mean square error on the validation set\n",
        "rmse_final = np.sqrt(float(hist['val_mean_squared_error'].tail(1)))\n",
        "\n",
        "# Print the final root mean square error\n",
        "print()\n",
        "print('Final Root Mean Square Error on validation set: {}'.format(round(rmse_final, 3)))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Oh no, we have run into our first problem...\n",
        "\n",
        "AttributeError: module 'tensorflow._api.v2.train' has no attribute 'AdamOptimizer'\n",
        "\n",
        "go to [Tensorflow-Keras](https://www.tensorflow.org/api_docs/python/tf) to see what the problem is with tf.train...\n",
        "\n",
        "oh yeah it doesn't exist anymore (clearly this example was older)\n",
        "It has since been update to be within tf.keras.optimizers\n",
        "\n",
        "**Replace the tf.train.AdamOptimizer() in the build_model() function with an approriate Adam one found on the tensor flow site [TF](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam)**\n",
        "\n",
        "Rerun this model then the training script again, we will find another issue.\n",
        "\n",
        "# Error 2\n",
        "\n",
        "KeyError: 'val_mean_squared_error'\n",
        "What does that mean\n",
        "\n",
        "It is suggesting that the line, seen below, has an error\n",
        "\n",
        "```\n",
        "rmse_final = np.sqrt(float(hist['val_mean_squared_error'].tail(1)))\n",
        "```\n",
        "\n",
        "it is looking for that value inside hist that is called val_mean_squared_error\n",
        "\n",
        "call the hist variable to see a print out of this variable\n"
      ],
      "metadata": {
        "id": "Kkk5GORFbC25"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hist\n"
      ],
      "metadata": {
        "id": "R-b3qCJB3EEC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Not that there is no val_mean_squared_error only a val mse.\n",
        "**Correct the above code and rerun.**"
      ],
      "metadata": {
        "id": "qEXT8McSb6lM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rmse_final = np.sqrt(float(hist['val_mse'].tail(1)))\n",
        "print()\n",
        "print('Final Root Mean Square Error on validation set: {}'.format(round(rmse_final, 3)))"
      ],
      "metadata": {
        "id": "w4nsHz8YcpOT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jr1LFNk6KaRl"
      },
      "source": [
        "Now, let's plot the loss function measure on the training and validation sets. The validation set is used to prevent overfitting ([learn more about it here](https://www.tensorflow.org/tutorials/keras/overfit_and_underfit)). However, because our network is small, the training convergence without noticeably overfitting the data as the plot shows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "v3qHyM56KaRm"
      },
      "outputs": [],
      "source": [
        "def plot_history():\n",
        "    plt.figure()\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Mean Square Error [Thousand Dollars$^2$]')\n",
        "    plt.plot(hist['epoch'], hist['mean_squared_error'], label='Train Error')\n",
        "    plt.plot(hist['epoch'], hist['val_mean_squared_error'], label = 'Val Error')\n",
        "    plt.legend()\n",
        "    plt.ylim([0,50])\n",
        "\n",
        "plot_history()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again we have an issue with the code and the naming of the mse terms,  replace the two above with mse"
      ],
      "metadata": {
        "id": "cdXVPgXYdXpB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to plot the training and validation mean square error\n",
        "def plot_history():\n",
        "    plt.figure()\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Mean Square Error [Thousand Dollars$^2$]')\n",
        "    plt.plot(hist['epoch'], hist['mse'], label='Train Error')\n",
        "    plt.plot(hist['epoch'], hist['val_mse'], label = 'Validation Error')\n",
        "    plt.legend()\n",
        "    plt.ylim([0,50])\n",
        "\n",
        "# Call the function to plot the training and validation mean square error\n",
        "plot_history()"
      ],
      "metadata": {
        "id": "YtYg0xjVeIf1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*This code defines a function called \"plot_history\" which plots the training and validation mean square error as a function of the epochs. The x-axis is labeled as \"Epoch\" and y-axis is labeled as \"Mean Square Error [Thousand Dollars$^2$]\". The function plots the training error by using the data from the 'mse' column of the hist dataframe and plots the validation error by using the data from the 'val_mse' column of the hist dataframe. The plot is labeled with a legend, and y-axis limit is set to [0,50]. The function is called at the end of the code to execute the plot.*"
      ],
      "metadata": {
        "id": "by1RyiX-sAzI"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWvvYqx_KaRm"
      },
      "source": [
        "Next, compare how the model performs on the test dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DWQAhffIKaRm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "756d90e6-2ea4-4b48-c535-0f3c05a5aa8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4/4 [==============================] - 0s 4ms/step - loss: 16.5975 - mae: 2.7258 - mse: 16.5975\n",
            "Root Mean Square Error on test set: 4.074\n"
          ]
        }
      ],
      "source": [
        "# Normalize the test features using the statistics from the training set\n",
        "test_features_norm = (test_features - train_mean) / train_std\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "mse, _, _ = model.evaluate(test_features_norm, test_labels)\n",
        "\n",
        "# Calculate the root mean square error\n",
        "rmse = np.sqrt(mse)\n",
        "\n",
        "# Print the root mean square error\n",
        "print('Root Mean Square Error on test set: {}'.format(round(rmse, 3)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hX0FoBzNKaRn"
      },
      "source": [
        "Compare the RMSE measure you get to the [Kaggle leaderboard](https://www.kaggle.com/c/boston-housing/leaderboard). An RMSE of 4 puts us in 20th place."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Improving the results\n",
        "Change the number of neurons from 20 to a higher and lower value and note the changes.\n",
        "\n",
        "Similairly change the optimiser from the tensorflow link used previous, to see if you can improve your result.\n",
        "\n",
        "See if you can find how to add learning rate modifications to the model also available in the API doc link on the tensorflow page given earlier.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "tcP1Bb2ghyz2"
      }
    }
  ]
}